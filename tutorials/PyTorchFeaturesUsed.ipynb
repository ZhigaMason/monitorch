{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee34c837-9500-4ed4-b5e1-7a9574dbaa98",
   "metadata": {},
   "source": [
    "## Notebook Description\n",
    "\n",
    "This notebook presents features used from PyTorch. They include but are not limited to TensorBoard, `named_children` and hooks for modules.\n",
    "\n",
    "## Loading Modules and Data\n",
    "\n",
    "We will use MNIST dataset for a demo of tensorboard and hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aee0ba03-72cd-4d43-8c07-049fa54fc686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5f3bc8-9754-44d5-aed7-0cd3da228e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "validation_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b6a0c8-c9e1-4579-b863-c4cebe413fd8",
   "metadata": {},
   "source": [
    "Let us plot several pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1444f90-944c-4002-b9a3-fcf381e38f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(3,3, figsize=(7,7))\n",
    "\n",
    "for i, j in [(i,j) for i in range(3) for j in range(3)]:\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    axes[i, j].set_title(label)\n",
    "    axes[i, j].axis('off')\n",
    "    axes[i, j].imshow(img[0], vmin=0, vmax=1, cmap='grey')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf6bf5-c126-4bea-8b27-8ab005beabdc",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "\n",
    "TensorBoard is a tool developed by TensorFlow team. PyTorch gives access to limited subset of its features. One of the benefits of tensorboard over MatPlotLib is builtin ability to update plots online. Same behaviour can be reached for MatPlotLib, but it requires redrawing figure or axis. To access TensorBoard off Jupyter IDE you must have tensorboard python module installed, then run `tensorboard --logdir runs` in `tutorials` directory.\n",
    "\n",
    "TensorBoard can be manipulated by `SummaryWriter`. TensorBoard saves all data and plots persistantly into folder by default it is `runs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "409ca11a-f0ed-4d1a-9968-68c716365fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84912395-9652-4cb6-83f5-493773778b4e",
   "metadata": {},
   "source": [
    "`SummaryWriter` has many methods to interact with the board. Here we will note two most significant ones `add_scalar` and `add_figure`, the first adds a numeric value to tag and the second adds a matplotlib figure. Tags are a way to organize TensorBoard, they follow two-storey hierarchy with slash sign as a delimiter. TensorBoard also autonomously adds timestamps to all records made on it and provides a filter to control what shall be displayed, so keeping track of your progress is very straughtforward.\n",
    "\n",
    "Bellow are examples of plotting simple functions, stochastic noise and a figure from above onto the board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c8a0df6-45fe-485b-9956-db5e35ea09e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    writer.add_scalar('Det/Linear', i, i)\n",
    "    writer.add_scalar('Det/Quad', i**2 - 4*i, i)\n",
    "    writer.add_scalar('Stoch/np.random', np.random.random(), i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7fe8a3c-0df1-4588-96de-b41ab5e92249",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,3, figsize=(7,7))\n",
    "\n",
    "for i, j in [(i,j) for i in range(3) for j in range(3)]:\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    axes[i, j].set_title(label)\n",
    "    axes[i, j].axis('off')\n",
    "    axes[i, j].imshow(img[0])\n",
    "\n",
    "writer.add_figure('EDA', fig)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78befc09-5090-4b1b-a42d-174b7fc390d6",
   "metadata": {},
   "source": [
    "TensorBoard can be viewed from within the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37afd2b6-ad50-45ae-a47d-6bdc7c65dc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5049fc6e9d27bcf6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5049fc6e9d27bcf6\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a534f91f-845c-44f6-b3c2-cb1504055eff",
   "metadata": {},
   "source": [
    "## Named Children & Hooks in PyTorch\n",
    "\n",
    "One of the executives principles of Monitorch is ease of use in already established code. We try to achieve it by leverging frameworks tools such as hooks and module children.\n",
    "\n",
    "Hooks are callbacks that are called whenever forward or bacward pass goes through the module. Methods `children` and `named_children` return iterator of submodules directly cotained in the module.\n",
    "\n",
    "Below we create several instances of neural networks and prepare data to train neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebceb614-7a88-4234-a085-8fe651d82eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(validation_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671666b5-a202-4448-86de-790f4f7fbadc",
   "metadata": {},
   "source": [
    "Here we define a convolutional neural net, its submodules will be only layers that have state, that is, linear or dropout. Activation layers are used in functionval style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "624abb0d-bca6-4eda-9855-2897f05c626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1    = nn.Conv2d(1, 16,  kernel_size=3)\n",
    "        self.conv2    = nn.Conv2d(16, 64, kernel_size=3)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        self.lin1     = nn.Linear(24*24*64, 128)\n",
    "        self.lin2     = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        t = F.relu(self.conv1(X))\n",
    "        t = self.dropout2(F.relu(self.conv2(t)))\n",
    "        t = t.flatten(start_dim=1)\n",
    "        t = F.relu(self.lin1(t))\n",
    "        t = F.softmax(self.lin2(t), dim=1)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c149a-2770-41fa-9bb5-36bddc033df4",
   "metadata": {},
   "source": [
    "Then we proceed by instantiating our custom CNN and we also create two more feed-forward networks for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87efd35d-27fd-4739-bed7-ac1d9372a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "mynet = MyNet()\n",
    "\n",
    "ffn = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "\n",
    "    nn.Sequential(\n",
    "        nn.Linear(28*28, 512),\n",
    "        nn.ReLU(),\n",
    "    ),\n",
    "\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(512, 512),\n",
    "        nn.ReLU(),\n",
    "    ),\n",
    "\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(512, 512),\n",
    "        nn.ReLU(),\n",
    "    ),\n",
    "\n",
    "    nn.Sequential(\n",
    "        nn.Linear(512, 10),\n",
    "        nn.Softmax(dim=1),\n",
    "    ),\n",
    ")\n",
    "\n",
    "small = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28, 10),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Linear(10, 10),\n",
    "    nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f337a217-f72c-466d-9d71-540ccfd82a0d",
   "metadata": {},
   "source": [
    "### Hooks\n",
    "\n",
    "PyTorch provides four callbacks' (hooks') flavour, differentiating between forward and backwards hooks (run on forward and backward pass respectivly) and ordinary and pre- hooks (run after and before the pass).\n",
    "\n",
    "Here we define two simple hooks to print out number of zeros in input and output during forward pass and to display norms of gradients on backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c666297-2387-4587-a01e-88091cc8bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def fw_hook(module, args, output) -> None:\n",
    "    input = args[0]\n",
    "    print(f\"Zeros in input:  {(input == 0).float().mean():.3%}\")\n",
    "    print(f\"Zeros in output: {(output == 0).float().mean():.3%}\")\n",
    "\n",
    "@torch.no_grad\n",
    "def bw_hook(module, grad_input, grad_output):\n",
    "    inp_norm = torch.mean(torch.tensor([torch.linalg.norm(grad) for grad in grad_input])).item() if grad_input[0] is not None else float('nan')\n",
    "    out_norm = torch.mean(torch.tensor([torch.linalg.norm(grad) for grad in grad_output])).item()\n",
    "    print(f\"Mean input norm  {inp_norm:.3f}\")\n",
    "    print(f\"Mean output norm {out_norm:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e5ea86-03ab-4563-ba70-628af4beec83",
   "metadata": {},
   "source": [
    "We will register the hooks on `small` network and run one forward and backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b93cb0d6-052d-4f51-b991-1ee4031bc695",
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_handle = small.register_forward_hook(fw_hook)\n",
    "bw_handle = small.register_full_backward_hook(bw_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc9adf9d-9763-4ce5-92b0-3f1a14087006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeros in input:  80.074%\n",
      "Zeros in output: 0.000%\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "pred = small(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3b8295a-7dff-44bf-a365-93404cd7ada8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean input norm  nan\n",
      "Mean output norm 0.168\n"
     ]
    }
   ],
   "source": [
    "loss = F.cross_entropy(pred, y)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf373669-deba-4382-92f0-8ae6cfb7eb77",
   "metadata": {},
   "source": [
    "Note that there is no gradient of whole net, because input to this module are data, so they have no gradient. To compute gradient of hidden layers we must use `children` method.\n",
    "\n",
    "To get rid of hooks we need to call `remove` from the handle returned by hook's registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52991f07-5508-4706-8673-c798827d45c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_handle.remove()\n",
    "bw_handle.remove()\n",
    "\n",
    "pred = small(x)\n",
    "\n",
    "loss = F.cross_entropy(pred, y)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8e84d4-8cb4-489d-90bb-4321fa59fdd3",
   "metadata": {},
   "source": [
    "### Child modules\n",
    "\n",
    "Children methods return iterators to all direct containees of parent container. For customly defined networks, such us above described `MyNet`, superclass `nn.Module` manages submodule registration during object's construction. Therefore any effects applied functionally will not be recorded and must be treated differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d39ad5db-dce0-4ec4-900c-56eec2e91c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 -> Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "conv2 -> Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "dropout2 -> Dropout(p=0.4, inplace=False)\n",
      "lin1 -> Linear(in_features=36864, out_features=128, bias=True)\n",
      "lin2 -> Linear(in_features=128, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, child in mynet.named_children():\n",
    "    print(name, '->', child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0944443b-3550-4ece-9928-718373c2b948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> Flatten(start_dim=1, end_dim=-1)\n",
      "1 -> Sequential(\n",
      "  (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "2 -> BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "3 -> Sequential(\n",
      "  (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "4 -> BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "5 -> Sequential(\n",
      "  (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "6 -> Sequential(\n",
      "  (0): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (1): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for name, child in ffn.named_children():\n",
    "    print(name, '->', child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987031ad-2aee-4997-8e57-ad7a4d8f9119",
   "metadata": {},
   "source": [
    "Let us define backward and forward hook that will print information not only about its arguments, but also the module that have called it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce0389f8-6b9e-4717-8ab0-f2426e0dbe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_fw_hook(name):\n",
    "    def f(module, args, out):\n",
    "        inp = args[0]\n",
    "        print(f\"(fw) {name}: Neuron activation {(out != 0).float().mean():.3%}\")\n",
    "    return f\n",
    "\n",
    "def named_bw_hook(name):\n",
    "    def f(module, grad_input, grad_output):\n",
    "        inp_norm = torch.mean(torch.tensor([torch.linalg.norm(grad) for grad in grad_input])).item() if grad_input[0] is not None else float('nan')\n",
    "        out_norm = torch.mean(torch.tensor([torch.linalg.norm(grad) for grad in grad_output])).item()\n",
    "        print(f\"(bw) {name}: Mean input norm  {inp_norm:.3g}\")\n",
    "        print(f\"(bw) {name}: Mean output norm {out_norm:.3g}\")\n",
    "        pass\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33591772-cf4c-49ae-9517-926bda8cc5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, child in mynet.named_children():\n",
    "    child.register_full_backward_hook(named_bw_hook(name))\n",
    "    child.register_forward_hook(named_fw_hook(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea6d752-ba26-4e3f-b0c0-f13fb631c0d6",
   "metadata": {},
   "source": [
    "Let us do One forward and one bacward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08ef2c45-5f99-4bc1-8bdd-efe2be2bf872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(fw) conv1: Neuron activation 100.000%\n",
      "(fw) conv2: Neuron activation 100.000%\n",
      "(fw) dropout2: Neuron activation 30.933%\n",
      "(fw) lin1: Neuron activation 100.000%\n",
      "(fw) lin2: Neuron activation 100.000%\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "r = mynet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e081c15c-d710-4adb-a9af-46075650a857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bw) lin2: Mean input norm  0.00986\n",
      "(bw) lin2: Mean output norm 0.0166\n",
      "(bw) lin1: Mean input norm  0.0041\n",
      "(bw) lin1: Mean output norm 0.0071\n",
      "(bw) dropout2: Mean input norm  0.00529\n",
      "(bw) dropout2: Mean output norm 0.0041\n",
      "(bw) conv2: Mean input norm  0.00218\n",
      "(bw) conv2: Mean output norm 0.0038\n",
      "(bw) conv1: Mean input norm  nan\n",
      "(bw) conv1: Mean output norm 0.00136\n"
     ]
    }
   ],
   "source": [
    "F.cross_entropy(r, y).backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a2794c-bdca-4074-9953-5589c10113a5",
   "metadata": {},
   "source": [
    "Next we will train our network for 5 batch-iterations with hooks installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb688319-70c4-424a-b92d-5bbc7fbe0aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New iteration\n",
      "(fw) conv1: Neuron activation 100.000%\n",
      "(fw) conv2: Neuron activation 100.000%\n",
      "(fw) dropout2: Neuron activation 31.011%\n",
      "(fw) lin1: Neuron activation 100.000%\n",
      "(fw) lin2: Neuron activation 100.000%\n",
      "(bw) lin2: Mean input norm  0.00996\n",
      "(bw) lin2: Mean output norm 0.017\n",
      "(bw) lin1: Mean input norm  0.00415\n",
      "(bw) lin1: Mean output norm 0.00719\n",
      "(bw) dropout2: Mean input norm  0.00536\n",
      "(bw) dropout2: Mean output norm 0.00415\n",
      "(bw) conv2: Mean input norm  0.00222\n",
      "(bw) conv2: Mean output norm 0.00386\n",
      "(bw) conv1: Mean input norm  nan\n",
      "(bw) conv1: Mean output norm 0.00139\n",
      "New iteration\n",
      "(fw) conv1: Neuron activation 100.000%\n",
      "(fw) conv2: Neuron activation 100.000%\n",
      "(fw) dropout2: Neuron activation 30.014%\n",
      "(fw) lin1: Neuron activation 100.000%\n",
      "(fw) lin2: Neuron activation 100.000%\n",
      "(bw) lin2: Mean input norm  0.0143\n",
      "(bw) lin2: Mean output norm 0.0242\n",
      "(bw) lin1: Mean input norm  0.00841\n",
      "(bw) lin1: Mean output norm 0.0102\n",
      "(bw) dropout2: Mean input norm  0.0109\n",
      "(bw) dropout2: Mean output norm 0.00841\n",
      "(bw) conv2: Mean input norm  0.00503\n",
      "(bw) conv2: Mean output norm 0.00833\n",
      "(bw) conv1: Mean input norm  nan\n",
      "(bw) conv1: Mean output norm 0.00335\n",
      "New iteration\n",
      "(fw) conv1: Neuron activation 100.000%\n",
      "(fw) conv2: Neuron activation 100.000%\n",
      "(fw) dropout2: Neuron activation 33.960%\n",
      "(fw) lin1: Neuron activation 100.000%\n",
      "(fw) lin2: Neuron activation 100.000%\n",
      "(bw) lin2: Mean input norm  0.00324\n",
      "(bw) lin2: Mean output norm 0.00541\n",
      "(bw) lin1: Mean input norm  0.00268\n",
      "(bw) lin1: Mean output norm 0.00213\n",
      "(bw) dropout2: Mean input norm  0.00347\n",
      "(bw) dropout2: Mean output norm 0.00268\n",
      "(bw) conv2: Mean input norm  0.00202\n",
      "(bw) conv2: Mean output norm 0.00297\n",
      "(bw) conv1: Mean input norm  nan\n",
      "(bw) conv1: Mean output norm 0.00153\n",
      "New iteration\n",
      "(fw) conv1: Neuron activation 100.000%\n",
      "(fw) conv2: Neuron activation 100.000%\n",
      "(fw) dropout2: Neuron activation 35.128%\n",
      "(fw) lin1: Neuron activation 100.000%\n",
      "(fw) lin2: Neuron activation 100.000%\n",
      "(bw) lin2: Mean input norm  0.00371\n",
      "(bw) lin2: Mean output norm 0.00562\n",
      "(bw) lin1: Mean input norm  0.00301\n",
      "(bw) lin1: Mean output norm 0.00216\n",
      "(bw) dropout2: Mean input norm  0.00389\n",
      "(bw) dropout2: Mean output norm 0.00301\n",
      "(bw) conv2: Mean input norm  0.00217\n",
      "(bw) conv2: Mean output norm 0.00328\n",
      "(bw) conv1: Mean input norm  nan\n",
      "(bw) conv1: Mean output norm 0.00158\n",
      "New iteration\n",
      "(fw) conv1: Neuron activation 100.000%\n",
      "(fw) conv2: Neuron activation 100.000%\n",
      "(fw) dropout2: Neuron activation 26.598%\n",
      "(fw) lin1: Neuron activation 100.000%\n",
      "(fw) lin2: Neuron activation 100.000%\n",
      "(bw) lin2: Mean input norm  0.0157\n",
      "(bw) lin2: Mean output norm 0.0258\n",
      "(bw) lin1: Mean input norm  0.0141\n",
      "(bw) lin1: Mean output norm 0.0091\n",
      "(bw) dropout2: Mean input norm  0.0182\n",
      "(bw) dropout2: Mean output norm 0.0141\n",
      "(bw) conv2: Mean input norm  0.00757\n",
      "(bw) conv2: Mean output norm 0.0117\n",
      "(bw) conv1: Mean input norm  nan\n",
      "(bw) conv1: Mean output norm 0.00511\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "optimizer = torch.optim.NAdam(\n",
    "    mynet.parameters()\n",
    ")\n",
    "for data, labels in islice(train_loader, 5):\n",
    "    print('New iteration')\n",
    "    optimizer.zero_grad()\n",
    "    pred = mynet(data)\n",
    "    loss = F.cross_entropy(pred, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f0c1ec-12b1-447b-9c8f-0817626636b3",
   "metadata": {},
   "source": [
    "We have obtained a mass of data, let us try it on even deeper net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78658bf6-2ee6-4757-9d80-c6ae9992beb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New iteration\n",
      "(fw) 0: Neuron activation 18.355%\n",
      "(fw) 1: Neuron activation 51.453%\n",
      "(fw) 2: Neuron activation 98.633%\n",
      "(fw) 3: Neuron activation 50.043%\n",
      "(fw) 4: Neuron activation 100.000%\n",
      "(fw) 5: Neuron activation 50.201%\n",
      "(fw) 6: Neuron activation 100.000%\n",
      "(bw) 6: Mean input norm  0.00961\n",
      "(bw) 6: Mean output norm 0.168\n",
      "(bw) 5: Mean input norm  0.00395\n",
      "(bw) 5: Mean output norm 0.00961\n",
      "(bw) 4: Mean input norm  0.0123\n",
      "(bw) 4: Mean output norm 0.00395\n",
      "(bw) 3: Mean input norm  0.00504\n",
      "(bw) 3: Mean output norm 0.0123\n",
      "(bw) 2: Mean input norm  0.22\n",
      "(bw) 2: Mean output norm 0.00504\n",
      "(bw) 1: Mean input norm  nan\n",
      "(bw) 1: Mean output norm 0.22\n",
      "New iteration\n",
      "(fw) 0: Neuron activation 17.877%\n",
      "(fw) 1: Neuron activation 51.086%\n",
      "(fw) 2: Neuron activation 100.000%\n",
      "(fw) 3: Neuron activation 49.323%\n",
      "(fw) 4: Neuron activation 100.000%\n",
      "(fw) 5: Neuron activation 49.512%\n",
      "(fw) 6: Neuron activation 100.000%\n",
      "(bw) 6: Mean input norm  0.0157\n",
      "(bw) 6: Mean output norm 0.166\n",
      "(bw) 5: Mean input norm  0.0068\n",
      "(bw) 5: Mean output norm 0.0157\n",
      "(bw) 4: Mean input norm  0.0195\n",
      "(bw) 4: Mean output norm 0.0068\n",
      "(bw) 3: Mean input norm  0.00867\n",
      "(bw) 3: Mean output norm 0.0195\n",
      "(bw) 2: Mean input norm  0.509\n",
      "(bw) 2: Mean output norm 0.00867\n",
      "(bw) 1: Mean input norm  nan\n",
      "(bw) 1: Mean output norm 0.509\n",
      "New iteration\n",
      "(fw) 0: Neuron activation 18.874%\n",
      "(fw) 1: Neuron activation 50.507%\n",
      "(fw) 2: Neuron activation 100.000%\n",
      "(fw) 3: Neuron activation 49.225%\n",
      "(fw) 4: Neuron activation 100.000%\n",
      "(fw) 5: Neuron activation 49.365%\n",
      "(fw) 6: Neuron activation 100.000%\n",
      "(bw) 6: Mean input norm  0.017\n",
      "(bw) 6: Mean output norm 0.161\n",
      "(bw) 5: Mean input norm  0.00822\n",
      "(bw) 5: Mean output norm 0.017\n",
      "(bw) 4: Mean input norm  0.0218\n",
      "(bw) 4: Mean output norm 0.00822\n",
      "(bw) 3: Mean input norm  0.0104\n",
      "(bw) 3: Mean output norm 0.0218\n",
      "(bw) 2: Mean input norm  0.774\n",
      "(bw) 2: Mean output norm 0.0104\n",
      "(bw) 1: Mean input norm  nan\n",
      "(bw) 1: Mean output norm 0.774\n",
      "New iteration\n",
      "(fw) 0: Neuron activation 18.363%\n",
      "(fw) 1: Neuron activation 51.788%\n",
      "(fw) 2: Neuron activation 100.000%\n",
      "(fw) 3: Neuron activation 49.463%\n",
      "(fw) 4: Neuron activation 100.000%\n",
      "(fw) 5: Neuron activation 48.755%\n",
      "(fw) 6: Neuron activation 100.000%\n",
      "(bw) 6: Mean input norm  0.0141\n",
      "(bw) 6: Mean output norm 0.161\n",
      "(bw) 5: Mean input norm  0.00767\n",
      "(bw) 5: Mean output norm 0.0141\n",
      "(bw) 4: Mean input norm  0.0202\n",
      "(bw) 4: Mean output norm 0.00767\n",
      "(bw) 3: Mean input norm  0.0105\n",
      "(bw) 3: Mean output norm 0.0202\n",
      "(bw) 2: Mean input norm  0.604\n",
      "(bw) 2: Mean output norm 0.0105\n",
      "(bw) 1: Mean input norm  nan\n",
      "(bw) 1: Mean output norm 0.604\n",
      "New iteration\n",
      "(fw) 0: Neuron activation 18.646%\n",
      "(fw) 1: Neuron activation 51.056%\n",
      "(fw) 2: Neuron activation 100.000%\n",
      "(fw) 3: Neuron activation 48.425%\n",
      "(fw) 4: Neuron activation 100.000%\n",
      "(fw) 5: Neuron activation 47.662%\n",
      "(fw) 6: Neuron activation 100.000%\n",
      "(bw) 6: Mean input norm  0.0165\n",
      "(bw) 6: Mean output norm 0.158\n",
      "(bw) 5: Mean input norm  0.00923\n",
      "(bw) 5: Mean output norm 0.0165\n",
      "(bw) 4: Mean input norm  0.0221\n",
      "(bw) 4: Mean output norm 0.00923\n",
      "(bw) 3: Mean input norm  0.0123\n",
      "(bw) 3: Mean output norm 0.0221\n",
      "(bw) 2: Mean input norm  0.796\n",
      "(bw) 2: Mean output norm 0.0123\n",
      "(bw) 1: Mean input norm  nan\n",
      "(bw) 1: Mean output norm 0.796\n"
     ]
    }
   ],
   "source": [
    "for name, child in ffn.named_children():\n",
    "    child.register_full_backward_hook(named_bw_hook(name))\n",
    "    child.register_forward_hook(named_fw_hook(name))\n",
    "\n",
    "optimizer = torch.optim.NAdam(\n",
    "    ffn.parameters()\n",
    ")\n",
    "\n",
    "for data, labels in islice(train_loader, 5):\n",
    "    print('New iteration')\n",
    "    optimizer.zero_grad()\n",
    "    pred = ffn(data)\n",
    "    loss = F.cross_entropy(pred, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d3288-0b7d-4c4c-a721-f78a26765bc8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook shows technological foundations of Monitorch. The library adds a number of little machines and metrics to monitor neural network training efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2677f6-f8a1-4a09-ae83-e22c68dfc628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
